{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb1508-583a-4d35-a494-d736f2d40b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # visualizing data\n",
    "import seaborn as sns # visualizing data with stunning default theme\n",
    "import sklearn # contain algorithms\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load dataset from input directory\n",
    "df = pd.read_csv(\"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train.csv\") \n",
    "df[df['age'].notna()].head()\n",
    "sns.set(rc={'figure.figsize':(15, 5)})\n",
    "sns.countplot(x=\"age\", \n",
    "        data=df[df['age'].notna()], \n",
    "        order=['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties', 'seventies', 'eighties'])\n",
    "\n",
    "plt.show()\n",
    "#del df['duration']\n",
    "start=df.shape\n",
    "#df.isna().sum()\n",
    "end = df[df['age'].notna()].shape\n",
    "\n",
    "print(\"initial: {} final: {}\".format(start, end))\n",
    "\n",
    "sns.countplot(x=\"age\", \n",
    "        data=df[df['age'].notna()],\n",
    "        order=['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties', 'seventies', 'eighties'])\n",
    "\n",
    "plt.show()\n",
    "#we extract the columns that we think useful are\n",
    "df = df[['filename','age']]\n",
    "#To clean the data we remove the sample with NaN attribute values.\n",
    "data = df[df['age'].notna()]\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data.head()\n",
    "import librosa\n",
    "ds_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/\"\n",
    "\n",
    "#this function is used to extract audio frequency features\n",
    "def feature_extraction(filename, sampling_rate=48000):\n",
    "    path = \"{}{}\".format(ds_path, filename)\n",
    "    features = list()\n",
    "    audio, _ = librosa.load(path, sr=sampling_rate)\n",
    "    \n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sampling_rate))\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sampling_rate))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate))\n",
    "    features.append(spectral_centroid)\n",
    "    features.append(spectral_bandwidth)\n",
    "    features.append(spectral_rolloff)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate)\n",
    "    for el in mfcc:\n",
    "        features.append(np.mean(el))\n",
    "    \n",
    "    return features\n",
    "    \n",
    "        \n",
    "features = feature_extraction(data.iloc[0]['filename'])\n",
    "print(\"features: \", features)\n",
    "#the function create dataframe to store the feature and label related to each other\n",
    "def create_df_features(orig):\n",
    "    new_rows = list()\n",
    "    tot_rows = len(orig)-1\n",
    "    stop_counter = 55001  # Stop after processing 55001 rows\n",
    "    \n",
    "    for idx, row in orig.iterrows():\n",
    "        if idx >= stop_counter: break\n",
    "        print(\"\\r\", end=\"\")\n",
    "        print(\"{}/{}\".format(idx, tot_rows), end=\"\", flush=True)\n",
    "        features = feature_extraction(row['filename'])\n",
    "        features.append(row['age'])\n",
    "        new_rows.append(features)\n",
    "\n",
    "    return pd.DataFrame(new_rows, columns=[\"spectral_centroid\", \"spectral_bandwidth\", \"spectral_rolloff\",\n",
    "                                    \"mfcc1\", \"mfcc2\", \"mfcc3\", \"mfcc4\", \"mfcc5\", \"mfcc6\", \"mfcc7\", \"mfcc8\",\n",
    "                                   \"mfcc9\", \"mfcc10\", \"mfcc11\", \"mfcc12\", \"mfcc13\", \"mfcc14\", \"mfcc15\", \"mfcc16\",\n",
    "                                   \"mfcc17\", \"mfcc18\", \"mfcc19\", \"mfcc20\", \"label\"])\n",
    "\n",
    "df_features = create_df_features(data)\n",
    "df_features.head()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(data):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(np.array(data.iloc[:, 0:-1], dtype = float))\n",
    "    # with data.iloc[:, 0:-1] we don't consider the label column\n",
    "        \n",
    "    return scaled_data, scaler\n",
    "\n",
    "x, scaler = scale_features(df_features)\n",
    "print(\"Before scaling:\", df_features.iloc[0].values[:-1])\n",
    "print(\"\\nAfter scaling:\", x[0])\n",
    "\n",
    "df_features['label'].unique()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_labels(data):\n",
    "    labels = data.iloc[:, -1]\n",
    "    encoder = LabelEncoder()\n",
    "    labels = encoder.fit_transform(labels)\n",
    "    return labels, encoder\n",
    "\n",
    "y, encoder = get_labels(df_features)\n",
    "classes = encoder.classes_\n",
    "print(\"Before encoding:\", df_features.iloc[0].values[-1])\n",
    "print(\"\\nAfter encoding:\", y[0])\n",
    "print(\"\\nClasses:\", classes)\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "n_features = 22\n",
    "\n",
    "f_selector = SelectKBest(f_classif, k=n_features).fit(x, y)\n",
    "X_new = f_selector.transform(x)\n",
    "scores = f_selector.scores_\n",
    "\n",
    "indices = np.argsort(scores)[::-1]\n",
    "\n",
    "features = []\n",
    "for i in range(n_features):\n",
    "    features.append(df_features.columns[indices[i]])\n",
    "    \n",
    "plt.figure(figsize=(22, 5))\n",
    "plt.bar(features, scores[indices[range(n_features)]], color='g')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.show()\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifiers_and_params = [\n",
    "    (SVC(), {'C': [200, 150, 100], 'gamma': ['auto', 'scale']}),\n",
    "    (RandomForestClassifier(), {'n_estimators': [100, 150, 200]})\n",
    "]\n",
    "\n",
    "for tup in classifiers_and_params:\n",
    "    print(\"{}\".format(tup[0].__class__.__name__))\n",
    "    \n",
    "    # the main CV process\n",
    "    outer_cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    fold_counter = 0\n",
    "\n",
    "    results = list()\n",
    "    for train_idx, test_idx in outer_cv.split(X_new):\n",
    "        fold_counter += 1\n",
    "        \n",
    "        # split data in training and test sets\n",
    "        X_train, X_test = X_new[train_idx], X_new[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # the CV process used for the Grid Search\n",
    "        inner_cv = KFold(n_splits=2, shuffle=True, random_state=0)\n",
    "\n",
    "        # define and run the Grid Search CV process\n",
    "        gs = GridSearchCV(tup[0], tup[1], scoring='f1_macro', cv=inner_cv, refit=True)\n",
    "        res = gs.fit(X_train, y_train)\n",
    "\n",
    "        # get the best model, re-fit on the whole training set\n",
    "        best_model = res.best_estimator_\n",
    "\n",
    "        # evaluation on the test set\n",
    "        pred = best_model.predict(X_test)\n",
    "        score = f1_score(y_test, pred, average='macro')\n",
    "        results.append(score)\n",
    "        \n",
    "        print(\"\\tFold {}, Best Params {} with F1 Score {:.3f}, F1 Score on Test data {:.3f}\"\n",
    "              .format(fold_counter, res.best_params_, res.best_score_, score))\n",
    "\n",
    "    print('\\tAverage F1 Score on Test Set: {:.3f}\\n'.format(np.mean(results)))\n",
    "    import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def my_plot_confusion_matrix(cm, classes, normalize=False):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = \"Normalized Confusion Matrix\"\n",
    "    else:\n",
    "        title = \"Confusion Matrix (without normalization)\"\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.title(title)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    fmt = \"{:0.2f}\" if normalize else \"{:d}\"\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, fmt.format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = SVC(C=100, gamma='scale')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "my_plot_confusion_matrix(cm, classes=classes)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "my_plot_confusion_matrix(cm, classes=classes, normalize=True)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "import joblib\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(model, filename)\n",
    "import joblib\n",
    "# Save all necessary objects\n",
    "joblib.dump(scaler, 'scaler.sav')\n",
    "joblib.dump(f_selector, 'feature_selector.sav')\n",
    "joblib.dump(encoder, 'label_encoder.sav')\n",
    "print(\"All model files saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
