{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1181e4-2051-4db6-9adc-f2f3da5c16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sampling rate: 48000 Hz\n",
      "Audio already at target sampling rate\n"
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "\n",
    "# def check_and_resample_audio(audio_path, target_sr=48000):\n",
    "#     # Load the audio file and get its original sampling rate\n",
    "#     audio, original_sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "#     print(f\"Original sampling rate: {original_sr} Hz\")\n",
    "    \n",
    "#     if original_sr != target_sr:\n",
    "#         print(f\"Resampling from {original_sr} Hz to {target_sr} Hz\")\n",
    "#         # Resample the audio\n",
    "#         audio_resampled = librosa.resample(y=audio, orig_sr=original_sr, target_sr=target_sr)\n",
    "        \n",
    "#         # Save the resampled audio\n",
    "#         output_path = audio_path.rsplit('.', 1)[0] + f'_resampled_{target_sr}hz.wav'\n",
    "#         librosa.save(output_path, audio_resampled, sr=target_sr)\n",
    "#         print(f\"Saved resampled audio to: {output_path}\")\n",
    "#         return output_path\n",
    "#     else:\n",
    "#         print(\"Audio already at target sampling rate\")\n",
    "#         return audio_path\n",
    "\n",
    "# # Example usage\n",
    "# audio_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"  # Replace with your audio file path\n",
    "# resampled_path = check_and_resample_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69c2cd0-35d6-4501-b7a6-2c496c6401f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /opt/anaconda3/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c5d7c2c-91aa-4fb1-b5d1-fceb884d08e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sampling rate: 48000 Hz\n",
      "Audio already at target sampling rate\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def check_and_resample_audio(audio_path, target_sr=48000):\n",
    "    # Load the audio file and get its original sampling rate\n",
    "    audio, original_sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    print(f\"Original sampling rate: {original_sr} Hz\")\n",
    "    \n",
    "    if original_sr != target_sr:\n",
    "        print(f\"Resampling from {original_sr} Hz to {target_sr} Hz\")\n",
    "        # Resample the audio\n",
    "        audio_resampled = librosa.resample(y=audio, orig_sr=original_sr, target_sr=target_sr)\n",
    "        \n",
    "        # Save the resampled audio using soundfile\n",
    "        output_path = audio_path.rsplit('.', 1)[0] + f'_resampled_{target_sr}hz.wav'\n",
    "        sf.write(output_path, audio_resampled, target_sr)\n",
    "        print(f\"Saved resampled audio to: {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        print(\"Audio already at target sampling rate\")\n",
    "        return audio_path\n",
    "\n",
    "# Example usage\n",
    "audio_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "resampled_path = check_and_resample_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70ca8e1-5d56-4092-a474-47f5247664e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sampling rate: 48000 Hz\n",
      "Audio already at target sampling rate\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict_new_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Process and predict\u001b[39;00m\n\u001b[1;32m      2\u001b[0m resampled_path \u001b[38;5;241m=\u001b[39m check_and_resample_audio(audio_path)\n\u001b[0;32m----> 3\u001b[0m predicted_age \u001b[38;5;241m=\u001b[39m predict_new_audio(resampled_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted age group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_age\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_new_audio' is not defined"
     ]
    }
   ],
   "source": [
    "# # Process and predict\n",
    "# resampled_path = check_and_resample_audio(audio_path)\n",
    "# predicted_age = predict_new_audio(resampled_path)\n",
    "# print(f\"Predicted age group: {predicted_age}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ccb7e3-948a-4550-93cd-4c90c48a3989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sampling rate: 48000 Hz\n",
      "Audio already at target sampling rate\n",
      "Error occurred: name 'scaler' is not defined\n"
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# def check_and_resample_audio(audio_path, target_sr=48000):\n",
    "#     # Load the audio file and get its original sampling rate\n",
    "#     audio, original_sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "#     print(f\"Original sampling rate: {original_sr} Hz\")\n",
    "    \n",
    "#     if original_sr != target_sr:\n",
    "#         print(f\"Resampling from {original_sr} Hz to {target_sr} Hz\")\n",
    "#         # Resample the audio\n",
    "#         audio_resampled = librosa.resample(y=audio, orig_sr=original_sr, target_sr=target_sr)\n",
    "        \n",
    "#         # Save the resampled audio using soundfile\n",
    "#         output_path = audio_path.rsplit('.', 1)[0] + f'_resampled_{target_sr}hz.wav'\n",
    "#         sf.write(output_path, audio_resampled, target_sr)\n",
    "#         print(f\"Saved resampled audio to: {output_path}\")\n",
    "#         return output_path\n",
    "#     else:\n",
    "#         print(\"Audio already at target sampling rate\")\n",
    "#         return audio_path\n",
    "\n",
    "# def extract_features_from_path(audio_path, sampling_rate=48000):\n",
    "#     features = list()\n",
    "#     audio, _ = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "#     # Extract the same features we used in training\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sampling_rate))\n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sampling_rate))\n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate))\n",
    "#     features.append(spectral_centroid)\n",
    "#     features.append(spectral_bandwidth)\n",
    "#     features.append(spectral_rolloff)\n",
    "    \n",
    "#     mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate)\n",
    "#     for el in mfcc:\n",
    "#         features.append(np.mean(el))\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# def predict_new_audio(audio_path, model_path='finalized_model.sav'):\n",
    "#     # Load the saved model\n",
    "#     loaded_model = joblib.load(model_path)\n",
    "    \n",
    "#     # Extract features\n",
    "#     features = extract_features_from_path(audio_path)\n",
    "    \n",
    "#     # Convert to numpy array and reshape\n",
    "#     features = np.array(features).reshape(1, -1)\n",
    "    \n",
    "#     # Scale the features using the same scaler\n",
    "#     scaled_features = scaler.transform(features)\n",
    "    \n",
    "#     # Select the same features we used in training\n",
    "#     selected_features = f_selector.transform(scaled_features)\n",
    "    \n",
    "#     # Make prediction\n",
    "#     prediction = loaded_model.predict(selected_features)\n",
    "    \n",
    "#     # Convert numeric prediction back to age category\n",
    "#     predicted_age = encoder.inverse_transform(prediction)[0]\n",
    "    \n",
    "#     return predicted_age\n",
    "\n",
    "# # Example usage\n",
    "# audio_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "# try:\n",
    "#     # First resample if needed\n",
    "#     resampled_path = check_and_resample_audio(audio_path)\n",
    "    \n",
    "#     # Then predict\n",
    "#     predicted_age = predict_new_audio(resampled_path)\n",
    "#     print(f\"Predicted age group: {predicted_age}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34e39c-063b-4055-9ff6-3d8b9b5aedc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c4bbf-c9c6-4d59-afd3-3df4c8387dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa3c07-9550-4665-b30b-eacd05bd0d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac239588-3bc8-4b56-a5a2-060406e8d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450b394a-02c0-48ca-94ff-3a790ee40aa3",
   "metadata": {},
   "source": [
    "the next one works for the voice from the training dataaa   (27/4/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3876b207-f29d-450a-a903-39223b2a1d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and preprocessing objects...\n",
      "Loading complete!\n",
      "\n",
      "Processing audio file...\n",
      "Extracting features from /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\n",
      "Scaling features...\n",
      "Selecting features...\n",
      "Making prediction...\n",
      "\n",
      "Predicted age group: teens\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load all saved objects\n",
    "print(\"Loading saved model and preprocessing objects...\")\n",
    "model = joblib.load('finalized_model.sav')\n",
    "scaler = joblib.load('scaler.sav')\n",
    "f_selector = joblib.load('feature_selector.sav')\n",
    "encoder = joblib.load('label_encoder.sav')\n",
    "print(\"Loading complete!\")\n",
    "\n",
    "def extract_features(audio_path, sampling_rate=48000):\n",
    "    print(f\"Extracting features from {audio_path}\")\n",
    "    features = list()\n",
    "    audio, _ = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "    # Extract the same features used in training\n",
    "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sampling_rate))\n",
    "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sampling_rate))\n",
    "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate))\n",
    "    features.append(spectral_centroid)\n",
    "    features.append(spectral_bandwidth)\n",
    "    features.append(spectral_rolloff)\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate)\n",
    "    for el in mfcc:\n",
    "        features.append(np.mean(el))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def predict_age(audio_path):\n",
    "    # Extract features\n",
    "    features = extract_features(audio_path)\n",
    "    \n",
    "    # Reshape features\n",
    "    features = np.array(features).reshape(1, -1)\n",
    "    \n",
    "    # Scale features\n",
    "    print(\"Scaling features...\")\n",
    "    scaled_features = scaler.transform(features)\n",
    "    \n",
    "    # Select features\n",
    "    print(\"Selecting features...\")\n",
    "    selected_features = f_selector.transform(scaled_features)\n",
    "    \n",
    "    # Predict\n",
    "    print(\"Making prediction...\")\n",
    "    prediction = model.predict(selected_features)\n",
    "    \n",
    "    # Convert prediction to age category\n",
    "    age_category = encoder.inverse_transform(prediction)[0]\n",
    "    \n",
    "    return age_category\n",
    "\n",
    "# Test the model\n",
    "test_audio_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "\n",
    "try:\n",
    "    print(\"\\nProcessing audio file...\")\n",
    "    predicted_age = predict_age(test_audio_path)\n",
    "    print(f\"\\nPredicted age group: {predicted_age}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "# Optional: Test multiple files\n",
    "# test_files = [\n",
    "   # \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\",\n",
    "    # Add more test file paths here\n",
    "# ]\n",
    "\n",
    "    # print(\"\\nTesting multiple files:\")\n",
    "    # for audio_file in test_files:\n",
    "    #     try:\n",
    "    #         print(f\"\\nProcessing: {audio_file}\")\n",
    "    #         predicted_age = predict_age(audio_file)\n",
    "    #         print(f\"Predicted age group: {predicted_age}\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error processing {audio_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d2cfca-edb3-4ced-bb02-c901e3563aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and preprocessing objects...\n",
      "Loading complete!\n",
      "\n",
      "Processing audio file...\n",
      "Extracting features from /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\n",
      "Scaling features...\n",
      "Selecting features...\n",
      "Making prediction...\n",
      "\n",
      "Predicted age group: teens\n",
      "\n",
      "Testing multiple files:\n",
      "\n",
      "Processing: /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\n",
      "Extracting features from /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\n",
      "Scaling features...\n",
      "Selecting features...\n",
      "Making prediction...\n",
      "Predicted age group: teens\n"
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Load all saved objects\n",
    "# print(\"Loading saved model and preprocessing objects...\")\n",
    "# model = joblib.load('finalized_model.sav')\n",
    "# scaler = joblib.load('scaler.sav')\n",
    "# f_selector = joblib.load('feature_selector.sav')\n",
    "# encoder = joblib.load('label_encoder.sav')\n",
    "# print(\"Loading complete!\")\n",
    "\n",
    "# # def extract_features(audio_path, sampling_rate=48000):\n",
    "#     print(f\"Extracting features from {audio_path}\")\n",
    "#     features = list()\n",
    "#     audio, _ = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "#     # Extract the same features used in training\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sampling_rate))\n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sampling_rate))\n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate))\n",
    "#     features.append(spectral_centroid)\n",
    "#     features.append(spectral_bandwidth)\n",
    "#     features.append(spectral_rolloff)\n",
    "    \n",
    "#     mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate)\n",
    "#     for el in mfcc:\n",
    "#         features.append(np.mean(el))\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# def predict_age(audio_path):\n",
    "#     # Extract features\n",
    "#     features = extract_features(audio_path)\n",
    "    \n",
    "#     # Reshape features\n",
    "#     features = np.array(features).reshape(1, -1)\n",
    "    \n",
    "#     # Scale features\n",
    "#     print(\"Scaling features...\")\n",
    "#     scaled_features = scaler.transform(features)\n",
    "    \n",
    "#     # Select features\n",
    "#     print(\"Selecting features...\")\n",
    "#     selected_features = f_selector.transform(scaled_features)\n",
    "    \n",
    "#     # Predict\n",
    "#     print(\"Making prediction...\")\n",
    "#     prediction = model.predict(selected_features)\n",
    "    \n",
    "#     # Convert prediction to age category\n",
    "#     age_category = encoder.inverse_transform(prediction)[0]\n",
    "    \n",
    "#     return age_category\n",
    "\n",
    "# # Test the model\n",
    "# test_audio_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "\n",
    "# try:\n",
    "#     print(\"\\nProcessing audio file...\")\n",
    "#     predicted_age = predict_age(test_audio_path)\n",
    "#     print(f\"\\nPredicted age group: {predicted_age}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "# # Optional: Test multiple files\n",
    "# test_files = [\n",
    "#     \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\",\n",
    "    \n",
    "#     # Add more test file paths here\n",
    "# ]\n",
    "\n",
    "# print(\"\\nTesting multiple files:\")\n",
    "# for audio_file in test_files:\n",
    "#     try:\n",
    "#         print(f\"\\nProcessing: {audio_file}\")\n",
    "#         predicted_age = predict_age(audio_file)\n",
    "#         print(f\"Predicted age group: {predicted_age}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {audio_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5510a-f41e-4316-86c7-d0c900540bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b83a7-43b9-4333-b64a-4be298333456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e77d4d1-06f9-49b9-af47-2b5273548319",
   "metadata": {},
   "outputs": [],
   "source": [
    "//// rat 4 tay : checking if features match or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e61ba21-4e58-4cf7-a591-4351d11e0b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and preprocessing objects...\n",
      "Loading complete!\n",
      "\n",
      "Comparing audio properties:\n",
      "\n",
      "Training file:\n",
      "Sampling rate: 48000\n",
      "Duration: 7.56s\n",
      "Shape: (362880,)\n",
      "\n",
      "Test file:\n",
      "Sampling rate: 24000\n",
      "Duration: 41.74s\n",
      "Shape: (1001664,)\n",
      "\n",
      "Comparing features:\n",
      "Extracting features from /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000001.mp3\n",
      "Extracting features from /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/new teenage voice wav/teenager-complaining-about-schoolwork-online-audio-convertercom-71994.wav\n",
      "\n",
      "Feature comparison before scaling:\n",
      "Feature 0:\n",
      "  Training: 4188.8437359565905\n",
      "  Testing:  2346.015192237695\n",
      "Feature 1:\n",
      "  Training: 3650.1851630386923\n",
      "  Testing:  2295.214677761932\n",
      "Feature 2:\n",
      "  Training: 8192.381435119887\n",
      "  Testing:  4514.638704318937\n",
      "Feature 3:\n",
      "  Training: -733.8486328125\n",
      "  Testing:  -466.55206298828125\n",
      "Feature 4:\n",
      "  Training: 93.38440704345703\n",
      "  Testing:  131.048828125\n",
      "Feature 5:\n",
      "  Training: 1.2232623100280762\n",
      "  Testing:  -3.394333600997925\n",
      "Feature 6:\n",
      "  Training: 26.191797256469727\n",
      "  Testing:  21.0683536529541\n",
      "Feature 7:\n",
      "  Training: -0.3215196132659912\n",
      "  Testing:  1.4627008438110352\n",
      "Feature 8:\n",
      "  Training: 11.569067001342773\n",
      "  Testing:  2.175278663635254\n",
      "Feature 9:\n",
      "  Training: -5.715122699737549\n",
      "  Testing:  -6.546324729919434\n",
      "Feature 10:\n",
      "  Training: -0.6184390783309937\n",
      "  Testing:  -24.24925994873047\n",
      "Feature 11:\n",
      "  Training: 2.1940901279449463\n",
      "  Testing:  -9.780147552490234\n",
      "Feature 12:\n",
      "  Training: -6.02708625793457\n",
      "  Testing:  -12.979157447814941\n",
      "Feature 13:\n",
      "  Training: 0.6102701425552368\n",
      "  Testing:  -8.334794998168945\n",
      "Feature 14:\n",
      "  Training: -3.883195638656616\n",
      "  Testing:  -8.757473945617676\n",
      "Feature 15:\n",
      "  Training: 1.69571053981781\n",
      "  Testing:  -10.055183410644531\n",
      "Feature 16:\n",
      "  Training: -3.6030831336975098\n",
      "  Testing:  -2.9908318519592285\n",
      "Feature 17:\n",
      "  Training: -0.3748268485069275\n",
      "  Testing:  -11.788124084472656\n",
      "Feature 18:\n",
      "  Training: -0.6930966377258301\n",
      "  Testing:  -9.479300498962402\n",
      "Feature 19:\n",
      "  Training: -2.459653854370117\n",
      "  Testing:  -4.389719486236572\n",
      "Feature 20:\n",
      "  Training: -1.0143871307373047\n",
      "  Testing:  -3.550121545791626\n",
      "Feature 21:\n",
      "  Training: -2.5960001945495605\n",
      "  Testing:  -3.628248691558838\n",
      "Feature 22:\n",
      "  Training: 0.07877792418003082\n",
      "  Testing:  -6.670466423034668\n",
      "\n",
      "Feature comparison after scaling:\n",
      "Feature 0:\n",
      "  Training: 1.884332880304395\n",
      "  Testing:  -0.4303607145511729\n",
      "Feature 1:\n",
      "  Training: 1.755697343205147\n",
      "  Testing:  -0.3460168613492257\n",
      "Feature 2:\n",
      "  Training: 2.030346470718174\n",
      "  Testing:  -0.35486558908273674\n",
      "Feature 3:\n",
      "  Training: -3.1907322678538197\n",
      "  Testing:  -0.2569354285982928\n",
      "Feature 4:\n",
      "  Training: -0.9191295349782073\n",
      "  Testing:  0.5639767613427423\n",
      "Feature 5:\n",
      "  Training: 0.231504398447019\n",
      "  Testing:  -0.0008363217995049613\n",
      "Feature 6:\n",
      "  Training: 0.2079834256455947\n",
      "  Testing:  -0.12738863588614033\n",
      "Feature 7:\n",
      "  Training: -0.6352819905143978\n",
      "  Testing:  -0.5063657746182102\n",
      "Feature 8:\n",
      "  Training: 0.29770936535380643\n",
      "  Testing:  -0.3761477260902281\n",
      "Feature 9:\n",
      "  Training: -0.34990537829558055\n",
      "  Testing:  -0.4325471918034084\n",
      "Feature 10:\n",
      "  Training: 0.2279629237126003\n",
      "  Testing:  -2.5434949833311666\n",
      "Feature 11:\n",
      "  Training: 0.7115250844505095\n",
      "  Testing:  -0.7356713738338468\n",
      "Feature 12:\n",
      "  Training: -0.09149562741757729\n",
      "  Testing:  -1.0682269084501297\n",
      "Feature 13:\n",
      "  Training: 0.6438615295063561\n",
      "  Testing:  -0.7115214241081269\n",
      "Feature 14:\n",
      "  Training: 0.09769653410202461\n",
      "  Testing:  -0.8187511617599688\n",
      "Feature 15:\n",
      "  Training: 0.7151435611836817\n",
      "  Testing:  -1.4897126583482225\n",
      "Feature 16:\n",
      "  Training: 0.3819385453219354\n",
      "  Testing:  0.5068989348270009\n",
      "Feature 17:\n",
      "  Training: 0.5559666111993063\n",
      "  Testing:  -1.8934659492514536\n",
      "Feature 18:\n",
      "  Training: 0.5728658625866259\n",
      "  Testing:  -1.509453718014222\n",
      "Feature 19:\n",
      "  Training: 0.23928985780152442\n",
      "  Testing:  -0.23093624734234572\n",
      "Feature 20:\n",
      "  Training: 0.1956105736269379\n",
      "  Testing:  -0.465879840953859\n",
      "Feature 21:\n",
      "  Training: -0.16943489055870242\n",
      "  Testing:  -0.40943853286220194\n",
      "Feature 22:\n",
      "  Training: 1.1475117771285295\n",
      "  Testing:  -0.7390372224369683\n",
      "\n",
      "Selected features comparison:\n",
      "Feature 0:\n",
      "  Training: 1.884332880304395\n",
      "  Testing:  -0.4303607145511729\n",
      "Feature 1:\n",
      "  Training: 1.755697343205147\n",
      "  Testing:  -0.3460168613492257\n",
      "Feature 2:\n",
      "  Training: 2.030346470718174\n",
      "  Testing:  -0.35486558908273674\n",
      "Feature 3:\n",
      "  Training: -3.1907322678538197\n",
      "  Testing:  -0.2569354285982928\n",
      "Feature 4:\n",
      "  Training: -0.9191295349782073\n",
      "  Testing:  0.5639767613427423\n",
      "Feature 5:\n",
      "  Training: 0.231504398447019\n",
      "  Testing:  -0.0008363217995049613\n",
      "Feature 6:\n",
      "  Training: 0.2079834256455947\n",
      "  Testing:  -0.12738863588614033\n",
      "Feature 7:\n",
      "  Training: -0.6352819905143978\n",
      "  Testing:  -0.5063657746182102\n",
      "Feature 8:\n",
      "  Training: 0.29770936535380643\n",
      "  Testing:  -0.3761477260902281\n",
      "Feature 9:\n",
      "  Training: -0.34990537829558055\n",
      "  Testing:  -0.4325471918034084\n",
      "Feature 10:\n",
      "  Training: 0.2279629237126003\n",
      "  Testing:  -2.5434949833311666\n",
      "Feature 11:\n",
      "  Training: 0.7115250844505095\n",
      "  Testing:  -0.7356713738338468\n",
      "Feature 12:\n",
      "  Training: -0.09149562741757729\n",
      "  Testing:  -1.0682269084501297\n",
      "Feature 13:\n",
      "  Training: 0.6438615295063561\n",
      "  Testing:  -0.7115214241081269\n",
      "Feature 14:\n",
      "  Training: 0.7151435611836817\n",
      "  Testing:  -1.4897126583482225\n",
      "Feature 15:\n",
      "  Training: 0.3819385453219354\n",
      "  Testing:  0.5068989348270009\n",
      "Feature 16:\n",
      "  Training: 0.5559666111993063\n",
      "  Testing:  -1.8934659492514536\n",
      "Feature 17:\n",
      "  Training: 0.5728658625866259\n",
      "  Testing:  -1.509453718014222\n",
      "Feature 18:\n",
      "  Training: 0.23928985780152442\n",
      "  Testing:  -0.23093624734234572\n",
      "Feature 19:\n",
      "  Training: 0.1956105736269379\n",
      "  Testing:  -0.465879840953859\n",
      "Feature 20:\n",
      "  Training: -0.16943489055870242\n",
      "  Testing:  -0.40943853286220194\n",
      "Feature 21:\n",
      "  Training: 1.1475117771285295\n",
      "  Testing:  -0.7390372224369683\n",
      "\n",
      "Predictions:\n",
      "Training file predicted as: fifties\n",
      "Test file predicted as: fourties\n"
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Load all saved objects\n",
    "# print(\"Loading saved model and preprocessing objects...\")\n",
    "# model = joblib.load('finalized_model.sav')\n",
    "# scaler = joblib.load('scaler.sav')\n",
    "# f_selector = joblib.load('feature_selector.sav')\n",
    "# encoder = joblib.load('label_encoder.sav')\n",
    "# print(\"Loading complete!\")\n",
    "\n",
    "# def compare_training_and_testing():\n",
    "#     # Get a sample from training data\n",
    "#     train_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000001.mp3\"\n",
    "#     test_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/new teenage voice wav/teenager-complaining-about-schoolwork-online-audio-convertercom-71994.wav\"\n",
    "    \n",
    "#     print(\"\\nComparing audio properties:\")\n",
    "#     # Training file\n",
    "#     train_audio, train_sr = librosa.load(train_path, sr=None)\n",
    "#     print(f\"\\nTraining file:\")\n",
    "#     print(f\"Sampling rate: {train_sr}\")\n",
    "#     print(f\"Duration: {len(train_audio)/train_sr:.2f}s\")\n",
    "#     print(f\"Shape: {train_audio.shape}\")\n",
    "    \n",
    "#     # Test file\n",
    "#     test_audio, test_sr = librosa.load(test_path, sr=None)\n",
    "#     print(f\"\\nTest file:\")\n",
    "#     print(f\"Sampling rate: {test_sr}\")\n",
    "#     print(f\"Duration: {len(test_audio)/test_sr:.2f}s\")\n",
    "#     print(f\"Shape: {test_audio.shape}\")\n",
    "    \n",
    "#     print(\"\\nComparing features:\")\n",
    "#     # Extract features from both files\n",
    "#     train_features = extract_features(train_path)\n",
    "#     test_features = extract_features(test_path)\n",
    "    \n",
    "#     print(\"\\nFeature comparison before scaling:\")\n",
    "#     for i in range(len(train_features)):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {train_features[i]}\")\n",
    "#         print(f\"  Testing:  {test_features[i]}\")\n",
    "    \n",
    "#     # Compare scaled features\n",
    "#     scaled_train = scaler.transform([train_features])\n",
    "#     scaled_test = scaler.transform([test_features])\n",
    "    \n",
    "#     print(\"\\nFeature comparison after scaling:\")\n",
    "#     for i in range(len(scaled_train[0])):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {scaled_train[0][i]}\")\n",
    "#         print(f\"  Testing:  {scaled_test[0][i]}\")\n",
    "    \n",
    "#     # Compare selected features\n",
    "#     selected_train = f_selector.transform(scaled_train)\n",
    "#     selected_test = f_selector.transform(scaled_test)\n",
    "    \n",
    "#     print(\"\\nSelected features comparison:\")\n",
    "#     for i in range(len(selected_train[0])):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {selected_train[0][i]}\")\n",
    "#         print(f\"  Testing:  {selected_test[0][i]}\")\n",
    "    \n",
    "#     return train_features, test_features, selected_train, selected_test\n",
    "\n",
    "# # Run comparison\n",
    "# train_features, test_features, selected_train, selected_test = compare_training_and_testing()\n",
    "\n",
    "# # Make predictions on both\n",
    "# train_pred = model.predict(selected_train)\n",
    "# test_pred = model.predict(selected_test)\n",
    "\n",
    "# print(\"\\nPredictions:\")\n",
    "# print(f\"Training file predicted as: {encoder.inverse_transform(train_pred)[0]}\")\n",
    "# print(f\"Test file predicted as: {encoder.inverse_transform(test_pred)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebbe0d8-a118-4ac0-baf7-fad571982d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and preprocessing objects...\n",
      "Loading complete!\n",
      "\n",
      "Comparing audio properties:\n",
      "\n",
      "Training file:\n",
      "Sampling rate: 48000\n",
      "Duration: 6.02s\n",
      "Shape: (289152,)\n",
      "\n",
      "Test file:\n",
      "Sampling rate: 48000\n",
      "Duration: 4.27s\n",
      "Shape: (205056,)\n",
      "\n",
      "Comparing features:\n",
      "\n",
      "Feature comparison before scaling:\n",
      "Feature 0:\n",
      "  Training: 3437.1459236252354\n",
      "  Testing:  3837.5283485831287\n",
      "Feature 1:\n",
      "  Training: 2572.8236800745644\n",
      "  Testing:  2971.4351601708795\n",
      "Feature 2:\n",
      "  Training: 5879.162057522124\n",
      "  Testing:  6982.972256857855\n",
      "Feature 3:\n",
      "  Training: -636.5744018554688\n",
      "  Testing:  -318.2347717285156\n",
      "Feature 4:\n",
      "  Training: 97.48164367675781\n",
      "  Testing:  104.40064239501953\n",
      "Feature 5:\n",
      "  Training: 8.90262508392334\n",
      "  Testing:  -7.52297306060791\n",
      "Feature 6:\n",
      "  Training: 26.95400047302246\n",
      "  Testing:  56.50428771972656\n",
      "Feature 7:\n",
      "  Training: 1.254257321357727\n",
      "  Testing:  -41.74435806274414\n",
      "Feature 8:\n",
      "  Training: -11.111105918884277\n",
      "  Testing:  16.768495559692383\n",
      "Feature 9:\n",
      "  Training: -13.331087112426758\n",
      "  Testing:  -8.228477478027344\n",
      "Feature 10:\n",
      "  Training: -0.9903782606124878\n",
      "  Testing:  -25.24793815612793\n",
      "Feature 11:\n",
      "  Training: -13.528036117553711\n",
      "  Testing:  5.928062915802002\n",
      "Feature 12:\n",
      "  Training: -12.335501670837402\n",
      "  Testing:  -6.417112350463867\n",
      "Feature 13:\n",
      "  Training: 7.002572536468506\n",
      "  Testing:  -1.7163691520690918\n",
      "Feature 14:\n",
      "  Training: -1.2195146083831787\n",
      "  Testing:  -2.042240858078003\n",
      "Feature 15:\n",
      "  Training: -7.594547748565674\n",
      "  Testing:  -4.9824652671813965\n",
      "Feature 16:\n",
      "  Training: -4.645937442779541\n",
      "  Testing:  -9.167109489440918\n",
      "Feature 17:\n",
      "  Training: -2.0226340293884277\n",
      "  Testing:  -0.17999698221683502\n",
      "Feature 18:\n",
      "  Training: -6.952275276184082\n",
      "  Testing:  -5.194628715515137\n",
      "Feature 19:\n",
      "  Training: -6.187740325927734\n",
      "  Testing:  4.0002665519714355\n",
      "Feature 20:\n",
      "  Training: -6.992959022521973\n",
      "  Testing:  0.6099379658699036\n",
      "Feature 21:\n",
      "  Training: -7.074611186981201\n",
      "  Testing:  -12.139730453491211\n",
      "Feature 22:\n",
      "  Training: -5.038304805755615\n",
      "  Testing:  -1.3802149295806885\n",
      "\n",
      "Feature comparison after scaling:\n",
      "Feature 0:\n",
      "  Training: 0.9401592525402801\n",
      "  Testing:  1.4430615098889399\n",
      "Feature 1:\n",
      "  Training: 0.08458645423034107\n",
      "  Testing:  0.7028784531484941\n",
      "Feature 2:\n",
      "  Training: 0.5301004882566115\n",
      "  Testing:  1.2459801981496006\n",
      "Feature 3:\n",
      "  Training: -2.123068618034349\n",
      "  Testing:  1.3709672436694305\n",
      "Feature 4:\n",
      "  Training: -0.7577932586231453\n",
      "  Testing:  -0.4853448771366946\n",
      "Feature 5:\n",
      "  Training: 0.617902150069405\n",
      "  Testing:  -0.20857452240590318\n",
      "Feature 6:\n",
      "  Training: 0.2578759752319718\n",
      "  Testing:  2.1921884259402757\n",
      "Feature 7:\n",
      "  Training: -0.5214265537030857\n",
      "  Testing:  -3.6282280490603975\n",
      "Feature 8:\n",
      "  Training: -1.329237592363329\n",
      "  Testing:  0.6706868776782496\n",
      "Feature 9:\n",
      "  Training: -1.1071185320435204\n",
      "  Testing:  -0.5997943214457582\n",
      "Feature 10:\n",
      "  Training: 0.18434134182482673\n",
      "  Testing:  -2.660621452600416\n",
      "Feature 11:\n",
      "  Training: -1.188638084714032\n",
      "  Testing:  1.1628099460117762\n",
      "Feature 12:\n",
      "  Training: -0.9777964864333937\n",
      "  Testing:  -0.14629234548726325\n",
      "Feature 13:\n",
      "  Training: 1.6124422567292847\n",
      "  Testing:  0.2913222031255382\n",
      "Feature 14:\n",
      "  Training: 0.5985141298513628\n",
      "  Testing:  0.4438275222398897\n",
      "Feature 15:\n",
      "  Training: -1.0280160532722222\n",
      "  Testing:  -0.5379030176776959\n",
      "Feature 16:\n",
      "  Training: 0.16909214622589805\n",
      "  Testing:  -0.7536783514792031\n",
      "Feature 17:\n",
      "  Training: 0.20232712571593353\n",
      "  Testing:  0.5977794728441771\n",
      "Feature 18:\n",
      "  Training: -0.9105518275995225\n",
      "  Testing:  -0.4939917401191969\n",
      "Feature 19:\n",
      "  Training: -0.6689919913834822\n",
      "  Testing:  1.8131343748275517\n",
      "Feature 20:\n",
      "  Training: -1.364003862261459\n",
      "  Testing:  0.6193440195231941\n",
      "Feature 21:\n",
      "  Training: -1.210737400330653\n",
      "  Testing:  -2.3884064172254353\n",
      "Feature 22:\n",
      "  Training: -0.282815354061775\n",
      "  Testing:  0.7396940471671908\n",
      "\n",
      "Selected features comparison:\n",
      "Feature 0:\n",
      "  Training: 0.9401592525402801\n",
      "  Testing:  1.4430615098889399\n",
      "Feature 1:\n",
      "  Training: 0.08458645423034107\n",
      "  Testing:  0.7028784531484941\n",
      "Feature 2:\n",
      "  Training: 0.5301004882566115\n",
      "  Testing:  1.2459801981496006\n",
      "Feature 3:\n",
      "  Training: -2.123068618034349\n",
      "  Testing:  1.3709672436694305\n",
      "Feature 4:\n",
      "  Training: -0.7577932586231453\n",
      "  Testing:  -0.4853448771366946\n",
      "Feature 5:\n",
      "  Training: 0.617902150069405\n",
      "  Testing:  -0.20857452240590318\n",
      "Feature 6:\n",
      "  Training: 0.2578759752319718\n",
      "  Testing:  2.1921884259402757\n",
      "Feature 7:\n",
      "  Training: -0.5214265537030857\n",
      "  Testing:  -3.6282280490603975\n",
      "Feature 8:\n",
      "  Training: -1.329237592363329\n",
      "  Testing:  0.6706868776782496\n",
      "Feature 9:\n",
      "  Training: -1.1071185320435204\n",
      "  Testing:  -0.5997943214457582\n",
      "Feature 10:\n",
      "  Training: 0.18434134182482673\n",
      "  Testing:  -2.660621452600416\n",
      "Feature 11:\n",
      "  Training: -1.188638084714032\n",
      "  Testing:  1.1628099460117762\n",
      "Feature 12:\n",
      "  Training: -0.9777964864333937\n",
      "  Testing:  -0.14629234548726325\n",
      "Feature 13:\n",
      "  Training: 1.6124422567292847\n",
      "  Testing:  0.2913222031255382\n",
      "Feature 14:\n",
      "  Training: -1.0280160532722222\n",
      "  Testing:  -0.5379030176776959\n",
      "Feature 15:\n",
      "  Training: 0.16909214622589805\n",
      "  Testing:  -0.7536783514792031\n",
      "Feature 16:\n",
      "  Training: 0.20232712571593353\n",
      "  Testing:  0.5977794728441771\n",
      "Feature 17:\n",
      "  Training: -0.9105518275995225\n",
      "  Testing:  -0.4939917401191969\n",
      "Feature 18:\n",
      "  Training: -0.6689919913834822\n",
      "  Testing:  1.8131343748275517\n",
      "Feature 19:\n",
      "  Training: -1.364003862261459\n",
      "  Testing:  0.6193440195231941\n",
      "Feature 20:\n",
      "  Training: -1.210737400330653\n",
      "  Testing:  -2.3884064172254353\n",
      "Feature 21:\n",
      "  Training: -0.282815354061775\n",
      "  Testing:  0.7396940471671908\n",
      "\n",
      "Predictions:\n",
      "Training file predicted as: teens\n",
      "Test file predicted as: teens\n"
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Load all saved objects\n",
    "# print(\"Loading saved model and preprocessing objects...\")\n",
    "# model = joblib.load('finalized_model.sav')\n",
    "# scaler = joblib.load('scaler.sav')\n",
    "# f_selector = joblib.load('feature_selector.sav')\n",
    "# encoder = joblib.load('label_encoder.sav')\n",
    "# print(\"Loading complete!\")\n",
    "\n",
    "# def extract_features(audio_path, sampling_rate=48000):\n",
    "#     \"\"\"Extract audio features - must match the training feature extraction exactly\"\"\"\n",
    "#     features = list()\n",
    "#     # Force sampling rate to match training data\n",
    "#     audio, _ = librosa.load(audio_path, sr=sampling_rate)\n",
    "    \n",
    "#     # Extract the same features used in training\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sampling_rate))\n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sampling_rate))\n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate))\n",
    "#     features.append(spectral_centroid)\n",
    "#     features.append(spectral_bandwidth)\n",
    "#     features.append(spectral_rolloff)\n",
    "    \n",
    "#     mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate)\n",
    "#     for el in mfcc:\n",
    "#         features.append(np.mean(el))\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# def compare_training_and_testing():\n",
    "#     # Get a sample from training data\n",
    "#     train_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000039.mp3\"\n",
    "#     test_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "    \n",
    "#     print(\"\\nComparing audio properties:\")\n",
    "#     # Training file\n",
    "#     train_audio, train_sr = librosa.load(train_path, sr=48000)  # Force 48kHz\n",
    "#     print(f\"\\nTraining file:\")\n",
    "#     print(f\"Sampling rate: {train_sr}\")\n",
    "#     print(f\"Duration: {len(train_audio)/train_sr:.2f}s\")\n",
    "#     print(f\"Shape: {train_audio.shape}\")\n",
    "    \n",
    "#     # Test file\n",
    "#     test_audio, test_sr = librosa.load(test_path, sr=48000)  # Force 48kHz\n",
    "#     print(f\"\\nTest file:\")\n",
    "#     print(f\"Sampling rate: {test_sr}\")\n",
    "#     print(f\"Duration: {len(test_audio)/test_sr:.2f}s\")\n",
    "#     print(f\"Shape: {test_audio.shape}\")\n",
    "    \n",
    "#     print(\"\\nComparing features:\")\n",
    "#     # Extract features from both files\n",
    "#     train_features = extract_features(train_path)\n",
    "#     test_features = extract_features(test_path)\n",
    "    \n",
    "#     print(\"\\nFeature comparison before scaling:\")\n",
    "#     for i in range(len(train_features)):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {train_features[i]}\")\n",
    "#         print(f\"  Testing:  {test_features[i]}\")\n",
    "    \n",
    "#     # Compare scaled features\n",
    "#     scaled_train = scaler.transform([train_features])\n",
    "#     scaled_test = scaler.transform([test_features])\n",
    "    \n",
    "#     print(\"\\nFeature comparison after scaling:\")\n",
    "#     for i in range(len(scaled_train[0])):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {scaled_train[0][i]}\")\n",
    "#         print(f\"  Testing:  {scaled_test[0][i]}\")\n",
    "    \n",
    "#     # Compare selected features\n",
    "#     selected_train = f_selector.transform(scaled_train)\n",
    "#     selected_test = f_selector.transform(scaled_test)\n",
    "    \n",
    "#     print(\"\\nSelected features comparison:\")\n",
    "#     for i in range(len(selected_train[0])):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {selected_train[0][i]}\")\n",
    "#         print(f\"  Testing:  {selected_test[0][i]}\")\n",
    "    \n",
    "#     return train_features, test_features, selected_train, selected_test\n",
    "\n",
    "# # Run comparison\n",
    "# train_features, test_features, selected_train, selected_test = compare_training_and_testing()\n",
    "\n",
    "# # Make predictions on both\n",
    "# train_pred = model.predict(selected_train)\n",
    "# test_pred = model.predict(selected_test)\n",
    "\n",
    "# print(\"\\nPredictions:\")\n",
    "# print(f\"Training file predicted as: {encoder.inverse_transform(train_pred)[0]}\")\n",
    "# print(f\"Test file predicted as: {encoder.inverse_transform(test_pred)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12b4e63-ca57-494d-8fc0-7512e31f142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and preprocessing objects...\n",
      "Loading complete!\n",
      "Original sampling rate: 48000 Hz\n",
      "Audio already at target sampling rate\n",
      "\n",
      "Extracting features with consistent parameters...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m resampled_path \u001b[38;5;241m=\u001b[39m check_and_resample_audio(audio_path)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracting features with consistent parameters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m train_features \u001b[38;5;241m=\u001b[39m extract_features(train_path)\n\u001b[1;32m     84\u001b[0m test_features \u001b[38;5;241m=\u001b[39m extract_features(test_path)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Rest of your comparison code...\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# (Keep the scaling and prediction code the same)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Run the comparison\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_path' is not defined"
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Load all saved objects\n",
    "# print(\"Loading saved model and preprocessing objects...\")\n",
    "# model = joblib.load('finalized_model.sav')\n",
    "# scaler = joblib.load('scaler.sav')\n",
    "# f_selector = joblib.load('feature_selector.sav')\n",
    "# encoder = joblib.load('label_encoder.sav')\n",
    "# print(\"Loading complete!\")\n",
    "\n",
    "# def extract_features(audio_path, sampling_rate=48000, duration=5.0):\n",
    "#     \"\"\"Extract audio features with consistent parameters\"\"\"\n",
    "#     features = list()\n",
    "    \n",
    "#     # Load audio with fixed duration and sampling rate\n",
    "#     audio, sr = librosa.load(audio_path, sr=sampling_rate, duration=duration)\n",
    "    \n",
    "#     # Normalize audio\n",
    "#     audio = librosa.util.normalize(audio)\n",
    "    \n",
    "#     # Extract features with consistent parameters\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "    \n",
    "#     features.extend([spectral_centroid, spectral_bandwidth, spectral_rolloff])\n",
    "    \n",
    "#     # Extract MFCCs with consistent parameters\n",
    "#     mfcc = librosa.feature.mfcc(\n",
    "#         y=audio, \n",
    "#         sr=sr, \n",
    "#         n_mfcc=20,\n",
    "#         n_fft=2048,\n",
    "#         hop_length=512,\n",
    "#         n_mels=128\n",
    "#     )\n",
    "    \n",
    "#     # Add mean of each MFCC coefficient\n",
    "#     features.extend([np.mean(m) for m in mfcc])\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# def compare_training_and_testing():\n",
    "#     # Paths\n",
    "#     train_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000039.mp3\"\n",
    "#     test_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "# import librosa\n",
    "# import soundfile as sf\n",
    "\n",
    "# def check_and_resample_audio(audio_path, target_sr=48000):\n",
    "#     # Load the audio file and get its original sampling rate\n",
    "#     audio, original_sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "#     print(f\"Original sampling rate: {original_sr} Hz\")\n",
    "    \n",
    "#     if original_sr != target_sr:\n",
    "#         print(f\"Resampling from {original_sr} Hz to {target_sr} Hz\")\n",
    "#         # Resample the audio\n",
    "#         audio_resampled = librosa.resample(y=audio, orig_sr=original_sr, target_sr=target_sr)\n",
    "        \n",
    "#         # Save the resampled audio using soundfile\n",
    "#         output_path = audio_path.rsplit('.', 1)[0] + f'_resampled_{target_sr}hz.wav'\n",
    "#         sf.write(output_path, audio_resampled, target_sr)\n",
    "#         print(f\"Saved resampled audio to: {output_path}\")\n",
    "#         return output_path\n",
    "#     else:\n",
    "#         print(\"Audio already at target sampling rate\")\n",
    "#         return audio_path\n",
    "\n",
    "# # Example usage\n",
    "# audio_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "# resampled_path = check_and_resample_audio(audio_path)\n",
    "    \n",
    "# print(\"\\nExtracting features with consistent parameters...\")\n",
    "# train_features = extract_features(train_path)\n",
    "# test_features = extract_features(test_path)\n",
    "    \n",
    "#     # Rest of your comparison code...\n",
    "#     # (Keep the scaling and prediction code the same)\n",
    "\n",
    "# # Run the comparison\n",
    "# train_features, test_features, selected_train, selected_test = compare_training_and_testing()\n",
    "\n",
    "# # Make predictions\n",
    "# train_pred = model.predict(selected_train)\n",
    "# test_pred = model.predict(selected_test)\n",
    "\n",
    "# print(\"\\nPredictions:\")\n",
    "# print(f\"Training file predicted as: {encoder.inverse_transform(train_pred)[0]}\")\n",
    "# print(f\"Test file predicted as: {encoder.inverse_transform(test_pred)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f6f8454-2d9e-4445-9962-b3373edcaa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and preprocessing objects...\n",
      "Loading complete!\n",
      "\n",
      "Extracting features with consistent parameters...\n",
      "\n",
      "Feature comparison before scaling:\n",
      "Feature 0:\n",
      "  Training: 3096.5946252268727\n",
      "  Testing:  3837.528352588845\n",
      "Feature 1:\n",
      "  Training: 2410.459027458063\n",
      "  Testing:  2971.4351849457785\n",
      "Feature 2:\n",
      "  Training: 5282.782515991471\n",
      "  Testing:  6982.972256857855\n",
      "Feature 3:\n",
      "  Training: -358.1888122558594\n",
      "  Testing:  -346.345458984375\n",
      "Feature 4:\n",
      "  Training: 107.5014419555664\n",
      "  Testing:  104.40064239501953\n",
      "Feature 5:\n",
      "  Training: 7.424291133880615\n",
      "  Testing:  -7.522975444793701\n",
      "Feature 6:\n",
      "  Training: 29.056079864501953\n",
      "  Testing:  56.50428771972656\n",
      "Feature 7:\n",
      "  Training: 2.5654194355010986\n",
      "  Testing:  -41.74435806274414\n",
      "Feature 8:\n",
      "  Training: -13.856975555419922\n",
      "  Testing:  16.768495559692383\n",
      "Feature 9:\n",
      "  Training: -14.979944229125977\n",
      "  Testing:  -8.228477478027344\n",
      "Feature 10:\n",
      "  Training: 0.5555796027183533\n",
      "  Testing:  -25.247940063476562\n",
      "Feature 11:\n",
      "  Training: -15.237489700317383\n",
      "  Testing:  5.928062915802002\n",
      "Feature 12:\n",
      "  Training: -14.529577255249023\n",
      "  Testing:  -6.417112827301025\n",
      "Feature 13:\n",
      "  Training: 7.746092319488525\n",
      "  Testing:  -1.716368556022644\n",
      "Feature 14:\n",
      "  Training: -1.5084835290908813\n",
      "  Testing:  -2.042241096496582\n",
      "Feature 15:\n",
      "  Training: -8.749788284301758\n",
      "  Testing:  -4.982463836669922\n",
      "Feature 16:\n",
      "  Training: -5.123301029205322\n",
      "  Testing:  -9.167109489440918\n",
      "Feature 17:\n",
      "  Training: -2.7290754318237305\n",
      "  Testing:  -0.17999659478664398\n",
      "Feature 18:\n",
      "  Training: -7.5095930099487305\n",
      "  Testing:  -5.194629192352295\n",
      "Feature 19:\n",
      "  Training: -6.415596961975098\n",
      "  Testing:  4.000266075134277\n",
      "Feature 20:\n",
      "  Training: -7.975045680999756\n",
      "  Testing:  0.6099381446838379\n",
      "Feature 21:\n",
      "  Training: -7.39292573928833\n",
      "  Testing:  -12.139730453491211\n",
      "Feature 22:\n",
      "  Training: -5.192984580993652\n",
      "  Testing:  -1.3802145719528198\n",
      "\n",
      "Feature comparison after scaling:\n",
      "Feature 0:\n",
      "  Training: 0.5124081674981114\n",
      "  Testing:  1.4430615149203387\n",
      "Feature 1:\n",
      "  Training: -0.167259693290678\n",
      "  Testing:  0.7028784915771962\n",
      "Feature 2:\n",
      "  Training: 0.14331658878367112\n",
      "  Testing:  1.2459801981496006\n",
      "Feature 3:\n",
      "  Training: 0.9324392150767203\n",
      "  Testing:  1.0624296314031152\n",
      "Feature 4:\n",
      "  Training: -0.36324515491024834\n",
      "  Testing:  -0.4853448771366946\n",
      "Feature 5:\n",
      "  Training: 0.5435177341663908\n",
      "  Testing:  -0.20857464236950643\n",
      "Feature 6:\n",
      "  Training: 0.3954745814779949\n",
      "  Testing:  2.1921884259402757\n",
      "Feature 7:\n",
      "  Training: -0.4266904702155553\n",
      "  Testing:  -3.6282280490603975\n",
      "Feature 8:\n",
      "  Training: -1.5262107048330906\n",
      "  Testing:  0.6706868776782496\n",
      "Feature 9:\n",
      "  Training: -1.2710552614924293\n",
      "  Testing:  -0.5997943214457582\n",
      "Feature 10:\n",
      "  Training: 0.3656535851778151\n",
      "  Testing:  -2.660621676297108\n",
      "Feature 11:\n",
      "  Training: -1.3952412302837283\n",
      "  Testing:  1.1628099460117762\n",
      "Feature 12:\n",
      "  Training: -1.2860531480400887\n",
      "  Testing:  -0.14629241248050248\n",
      "Feature 13:\n",
      "  Training: 1.7251025922021337\n",
      "  Testing:  0.29132229344027394\n",
      "Feature 14:\n",
      "  Training: 0.5441830299672837\n",
      "  Testing:  0.44382747741311873\n",
      "Feature 15:\n",
      "  Training: -1.2447773729650833\n",
      "  Testing:  -0.5379027492664469\n",
      "Feature 16:\n",
      "  Training: 0.07166231394210082\n",
      "  Testing:  -0.7536783514792031\n",
      "Feature 17:\n",
      "  Training: 0.05071619879441854\n",
      "  Testing:  0.5977795559914136\n",
      "Feature 18:\n",
      "  Training: -1.0426354496008547\n",
      "  Testing:  -0.4939918531290208\n",
      "Feature 19:\n",
      "  Training: -0.7245052012612846\n",
      "  Testing:  1.8131342586546735\n",
      "Feature 20:\n",
      "  Training: -1.620198244060241\n",
      "  Testing:  0.6193440661699181\n",
      "Feature 21:\n",
      "  Training: -1.2847473428718745\n",
      "  Testing:  -2.3884064172254353\n",
      "Feature 22:\n",
      "  Training: -0.3260514498698576\n",
      "  Testing:  0.7396941471313443\n",
      "\n",
      "Selected features comparison:\n",
      "Feature 0:\n",
      "  Training: 0.5124081674981114\n",
      "  Testing:  1.4430615149203387\n",
      "Feature 1:\n",
      "  Training: -0.167259693290678\n",
      "  Testing:  0.7028784915771962\n",
      "Feature 2:\n",
      "  Training: 0.14331658878367112\n",
      "  Testing:  1.2459801981496006\n",
      "Feature 3:\n",
      "  Training: 0.9324392150767203\n",
      "  Testing:  1.0624296314031152\n",
      "Feature 4:\n",
      "  Training: -0.36324515491024834\n",
      "  Testing:  -0.4853448771366946\n",
      "Feature 5:\n",
      "  Training: 0.5435177341663908\n",
      "  Testing:  -0.20857464236950643\n",
      "Feature 6:\n",
      "  Training: 0.3954745814779949\n",
      "  Testing:  2.1921884259402757\n",
      "Feature 7:\n",
      "  Training: -0.4266904702155553\n",
      "  Testing:  -3.6282280490603975\n",
      "Feature 8:\n",
      "  Training: -1.5262107048330906\n",
      "  Testing:  0.6706868776782496\n",
      "Feature 9:\n",
      "  Training: -1.2710552614924293\n",
      "  Testing:  -0.5997943214457582\n",
      "Feature 10:\n",
      "  Training: 0.3656535851778151\n",
      "  Testing:  -2.660621676297108\n",
      "Feature 11:\n",
      "  Training: -1.3952412302837283\n",
      "  Testing:  1.1628099460117762\n",
      "Feature 12:\n",
      "  Training: -1.2860531480400887\n",
      "  Testing:  -0.14629241248050248\n",
      "Feature 13:\n",
      "  Training: 1.7251025922021337\n",
      "  Testing:  0.29132229344027394\n",
      "Feature 14:\n",
      "  Training: -1.2447773729650833\n",
      "  Testing:  -0.5379027492664469\n",
      "Feature 15:\n",
      "  Training: 0.07166231394210082\n",
      "  Testing:  -0.7536783514792031\n",
      "Feature 16:\n",
      "  Training: 0.05071619879441854\n",
      "  Testing:  0.5977795559914136\n",
      "Feature 17:\n",
      "  Training: -1.0426354496008547\n",
      "  Testing:  -0.4939918531290208\n",
      "Feature 18:\n",
      "  Training: -0.7245052012612846\n",
      "  Testing:  1.8131342586546735\n",
      "Feature 19:\n",
      "  Training: -1.620198244060241\n",
      "  Testing:  0.6193440661699181\n",
      "Feature 20:\n",
      "  Training: -1.2847473428718745\n",
      "  Testing:  -2.3884064172254353\n",
      "Feature 21:\n",
      "  Training: -0.3260514498698576\n",
      "  Testing:  0.7396941471313443\n",
      "\n",
      "Predictions:\n",
      "Training file predicted as: twenties\n",
      "Test file predicted as: teens\n"
     ]
    }
   ],
   "source": [
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Load all saved objects\n",
    "# print(\"Loading saved model and preprocessing objects...\")\n",
    "# model = joblib.load('finalized_model.sav')\n",
    "# scaler = joblib.load('scaler.sav')\n",
    "# f_selector = joblib.load('feature_selector.sav')\n",
    "# encoder = joblib.load('label_encoder.sav')\n",
    "# print(\"Loading complete!\")\n",
    "\n",
    "# def extract_features(audio_path, sampling_rate=48000, duration=5.0):\n",
    "#     \"\"\"Extract audio features with consistent parameters\"\"\"\n",
    "#     features = list()\n",
    "    \n",
    "#     # Load audio with fixed duration and sampling rate\n",
    "#     audio, sr = librosa.load(audio_path, sr=sampling_rate, duration=duration)\n",
    "    \n",
    "#     # Normalize audio\n",
    "#     audio = librosa.util.normalize(audio)\n",
    "    \n",
    "#     # Extract features with consistent parameters\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "    \n",
    "#     features.extend([spectral_centroid, spectral_bandwidth, spectral_rolloff])\n",
    "    \n",
    "#     # Extract MFCCs with consistent parameters\n",
    "#     mfcc = librosa.feature.mfcc(\n",
    "#         y=audio, \n",
    "#         sr=sr, \n",
    "#         n_mfcc=20,\n",
    "#         n_fft=2048,\n",
    "#         hop_length=512,\n",
    "#         n_mels=128\n",
    "#     )\n",
    "    \n",
    "#     # Add mean of each MFCC coefficient\n",
    "#     features.extend([np.mean(m) for m in mfcc])\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# def compare_training_and_testing():\n",
    "#     # Paths\n",
    "#     train_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000039.mp3\"\n",
    "#     test_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "#     print(\"\\nExtracting features with consistent parameters...\")\n",
    "#     train_features = extract_features(train_path)\n",
    "#     test_features = extract_features(test_path)\n",
    "    \n",
    "#     print(\"\\nFeature comparison before scaling:\")\n",
    "#     for i in range(len(train_features)):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {train_features[i]}\")\n",
    "#         print(f\"  Testing:  {test_features[i]}\")\n",
    "    \n",
    "#     # Compare scaled features\n",
    "#     scaled_train = scaler.transform([train_features])\n",
    "#     scaled_test = scaler.transform([test_features])\n",
    "    \n",
    "#     print(\"\\nFeature comparison after scaling:\")\n",
    "#     for i in range(len(scaled_train[0])):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {scaled_train[0][i]}\")\n",
    "#         print(f\"  Testing:  {scaled_test[0][i]}\")\n",
    "    \n",
    "#     # Compare selected features\n",
    "#     selected_train = f_selector.transform(scaled_train)\n",
    "#     selected_test = f_selector.transform(scaled_test)\n",
    "    \n",
    "#     print(\"\\nSelected features comparison:\")\n",
    "#     for i in range(len(selected_train[0])):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Training: {selected_train[0][i]}\")\n",
    "#         print(f\"  Testing:  {selected_test[0][i]}\")\n",
    "    \n",
    "#     return train_features, test_features, selected_train, selected_test\n",
    "\n",
    "# # Run the comparison\n",
    "# train_features, test_features, selected_train, selected_test = compare_training_and_testing()\n",
    "\n",
    "# # Make predictions\n",
    "# train_pred = model.predict(selected_train)\n",
    "# test_pred = model.predict(selected_test)\n",
    "\n",
    "# print(\"\\nPredictions:\")\n",
    "# print(f\"Training file predicted as: {encoder.inverse_transform(train_pred)[0]}\")\n",
    "# print(f\"Test file predicted as: {encoder.inverse_transform(test_pred)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d513e1-7573-4a9c-9ee3-662ef0faadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average teen features from training:\n",
      "Feature 0: 0.6311667786030105\n",
      "Feature 1: 0.30170929702936505\n",
      "Feature 2: 0.5507226146444031\n",
      "Feature 3: 0.880516897031169\n",
      "Feature 4: -0.4199579123055708\n",
      "Feature 5: -0.33528552706217013\n",
      "Feature 6: 0.2579611802392266\n",
      "Feature 7: -0.3449823542912598\n",
      "Feature 8: -0.43281442249545754\n",
      "Feature 9: -0.3735886428802297\n",
      "Feature 10: -0.16350530325154466\n",
      "Feature 11: -0.057971494395754504\n",
      "Feature 12: -0.01904391304034866\n",
      "Feature 13: -0.05481693173522997\n",
      "Feature 14: -0.36051010084457524\n",
      "Feature 15: -0.25996790043379797\n",
      "Feature 16: 0.5076404345478065\n",
      "Feature 17: -0.059776868832702564\n",
      "Feature 18: -0.05591900620928332\n",
      "Feature 19: -0.3045119921485256\n",
      "Feature 20: -0.4915625140229025\n",
      "Feature 21: -0.11689499599760786\n",
      "\n",
      "Difference from teen average:\n",
      "Feature 0 difference: 0.8118947363173282\n",
      "Feature 1 difference: 0.4011691945478311\n",
      "Feature 2 difference: 0.6952575835051975\n",
      "Feature 3 difference: 0.18191273437194622\n",
      "Feature 4 difference: 0.06538696483112383\n",
      "Feature 5 difference: 0.1267108846926637\n",
      "Feature 6 difference: 1.9342272457010492\n",
      "Feature 7 difference: 3.2832456947691377\n",
      "Feature 8 difference: 1.1035013001737073\n",
      "Feature 9 difference: 0.22620567856552848\n",
      "Feature 10 difference: 2.4971163730455634\n",
      "Feature 11 difference: 1.2207814404075308\n",
      "Feature 12 difference: 0.12724849944015382\n",
      "Feature 13 difference: 0.3461392251755039\n",
      "Feature 14 difference: 0.17739264842187163\n",
      "Feature 15 difference: 0.4937104510454052\n",
      "Feature 16 difference: 0.09013912144360714\n",
      "Feature 17 difference: 0.4342149842963182\n",
      "Feature 18 difference: 1.8690532648639568\n",
      "Feature 19 difference: 0.9238560583184436\n",
      "Feature 20 difference: 1.8968439032025328\n",
      "Feature 21 difference: 0.8565891431289522\n",
      "\n",
      "Predicted age group: teens\n"
     ]
    }
   ],
   "source": [
    "# # Add this to check training data distribution\n",
    "# def analyze_training_data():\n",
    "#     # Load a few known teen samples from training\n",
    "#     teen_samples = [\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000039.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000159.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000184.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000229.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000274.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000385.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000424.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000432.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000653.mp3\",\n",
    "#         \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000741.mp3\",\n",
    "        \n",
    "#         # Add a few more known teen samples\n",
    "#     ]\n",
    "    \n",
    "#     features_list = []\n",
    "#     for sample in teen_samples:\n",
    "#         features = extract_features(sample, duration=5.0)\n",
    "#         scaled = scaler.transform([features])\n",
    "#         selected = f_selector.transform(scaled)\n",
    "#         features_list.append(selected[0])\n",
    "    \n",
    "#     # Print average features for teen voices\n",
    "#     avg_features = np.mean(features_list, axis=0)\n",
    "#     print(\"Average teen features from training:\")\n",
    "#     for i, feat in enumerate(avg_features):\n",
    "#         print(f\"Feature {i}: {feat}\")\n",
    "#     return avg_features\n",
    "\n",
    "# # Compare test file with teen average\n",
    "# def analyze_test_file(test_path, teen_avg):\n",
    "#     test_features = extract_features(test_path, duration=5.0)\n",
    "#     scaled_test = scaler.transform([test_features])\n",
    "#     selected_test = f_selector.transform(scaled_test)\n",
    "    \n",
    "#     print(\"\\nDifference from teen average:\")\n",
    "#     for i in range(len(selected_test[0])):\n",
    "#         diff = abs(selected_test[0][i] - teen_avg[i])\n",
    "#         print(f\"Feature {i} difference: {diff}\")\n",
    "    \n",
    "#     return selected_test\n",
    "\n",
    "# # Add preprocessing steps\n",
    "# def preprocess_audio(audio_path, duration=5.0):\n",
    "#     \"\"\"Apply preprocessing to make test file more similar to training data\"\"\"\n",
    "#     y, sr = librosa.load(audio_path, sr=48000, duration=duration)\n",
    "    \n",
    "#     # Normalize amplitude\n",
    "#     y = librosa.util.normalize(y)\n",
    "    \n",
    "#     # Apply noise reduction\n",
    "#     y = librosa.effects.preemphasis(y)\n",
    "    \n",
    "#     # Trim silence\n",
    "#     y, _ = librosa.effects.trim(y, top_db=20)\n",
    "    \n",
    "#     return y, sr\n",
    "\n",
    "# # Modify feature extraction to use preprocessed audio\n",
    "# def extract_features_enhanced(audio_path, sampling_rate=48000, duration=5.0):\n",
    "#     # Preprocess audio\n",
    "#     audio, sr = preprocess_audio(audio_path, duration)\n",
    "    \n",
    "#     features = list()\n",
    "    \n",
    "#     # Extract features with enhanced parameters\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "    \n",
    "#     features.extend([spectral_centroid, spectral_bandwidth, spectral_rolloff])\n",
    "    \n",
    "#     # Extract MFCCs with teen-voice-specific parameters\n",
    "#     mfcc = librosa.feature.mfcc(\n",
    "#         y=audio, \n",
    "#         sr=sr, \n",
    "#         n_mfcc=20,\n",
    "#         n_fft=2048,\n",
    "#         hop_length=512,\n",
    "#         n_mels=128,\n",
    "#         fmin=100,  # Focus on typical teen voice frequency range\n",
    "#         fmax=8000\n",
    "#     )\n",
    "    \n",
    "#     features.extend([np.mean(m) for m in mfcc])\n",
    "#     return features\n",
    "\n",
    "# # Test with enhanced processing\n",
    "# test_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "\n",
    "# teen_avg = analyze_training_data()\n",
    "# test_features = analyze_test_file(test_path, teen_avg)\n",
    "\n",
    "# # Make prediction with enhanced features\n",
    "# pred = model.predict(test_features)\n",
    "# print(f\"\\nPredicted age group: {encoder.inverse_transform(pred)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b998b-5884-451d-8d01-1d12925ba601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f13fb1-014a-4b8a-be64-28902e922b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49552be8-df2d-44d3-a0c1-ce6b2897cb71",
   "metadata": {},
   "source": [
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12  (eng).wav     etay teens asche\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "626e969d-675a-483a-8eb4-dd1d9ab3c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with synchronized parameters...\n",
      "Average teen features from training:\n",
      "Feature 0: 0.6311667786030105\n",
      "Feature 1: 0.30170929702936505\n",
      "Feature 2: 0.5507226146444031\n",
      "Feature 3: 0.880516897031169\n",
      "Feature 4: -0.4199579123055708\n",
      "Feature 5: -0.33528552706217013\n",
      "Feature 6: 0.2579611802392266\n",
      "Feature 7: -0.3449823542912598\n",
      "Feature 8: -0.43281442249545754\n",
      "Feature 9: -0.3735886428802297\n",
      "Feature 10: -0.16350530325154466\n",
      "Feature 11: -0.057971494395754504\n",
      "Feature 12: -0.01904391304034866\n",
      "Feature 13: -0.05481693173522997\n",
      "Feature 14: -0.36051010084457524\n",
      "Feature 15: -0.25996790043379797\n",
      "Feature 16: 0.5076404345478065\n",
      "Feature 17: -0.059776868832702564\n",
      "Feature 18: -0.05591900620928332\n",
      "Feature 19: -0.3045119921485256\n",
      "Feature 20: -0.4915625140229025\n",
      "Feature 21: -0.11689499599760786\n",
      "\n",
      "Feature values comparison:\n",
      "Feature 0:\n",
      "  Test file: 1.4431\n",
      "  Teen avg:  0.6312\n",
      "  Difference: 0.8119\n",
      "Feature 1:\n",
      "  Test file: 0.7029\n",
      "  Teen avg:  0.3017\n",
      "  Difference: 0.4012\n",
      "Feature 2:\n",
      "  Test file: 1.2460\n",
      "  Teen avg:  0.5507\n",
      "  Difference: 0.6953\n",
      "Feature 3:\n",
      "  Test file: 1.0624\n",
      "  Teen avg:  0.8805\n",
      "  Difference: 0.1819\n",
      "Feature 4:\n",
      "  Test file: -0.4853\n",
      "  Teen avg:  -0.4200\n",
      "  Difference: 0.0654\n",
      "Feature 5:\n",
      "  Test file: -0.2086\n",
      "  Teen avg:  -0.3353\n",
      "  Difference: 0.1267\n",
      "Feature 6:\n",
      "  Test file: 2.1922\n",
      "  Teen avg:  0.2580\n",
      "  Difference: 1.9342\n",
      "Feature 7:\n",
      "  Test file: -3.6282\n",
      "  Teen avg:  -0.3450\n",
      "  Difference: 3.2832\n",
      "Feature 8:\n",
      "  Test file: 0.6707\n",
      "  Teen avg:  -0.4328\n",
      "  Difference: 1.1035\n",
      "Feature 9:\n",
      "  Test file: -0.5998\n",
      "  Teen avg:  -0.3736\n",
      "  Difference: 0.2262\n",
      "Feature 10:\n",
      "  Test file: -2.6606\n",
      "  Teen avg:  -0.1635\n",
      "  Difference: 2.4971\n",
      "Feature 11:\n",
      "  Test file: 1.1628\n",
      "  Teen avg:  -0.0580\n",
      "  Difference: 1.2208\n",
      "Feature 12:\n",
      "  Test file: -0.1463\n",
      "  Teen avg:  -0.0190\n",
      "  Difference: 0.1272\n",
      "Feature 13:\n",
      "  Test file: 0.2913\n",
      "  Teen avg:  -0.0548\n",
      "  Difference: 0.3461\n",
      "Feature 14:\n",
      "  Test file: -0.5379\n",
      "  Teen avg:  -0.3605\n",
      "  Difference: 0.1774\n",
      "Feature 15:\n",
      "  Test file: -0.7537\n",
      "  Teen avg:  -0.2600\n",
      "  Difference: 0.4937\n",
      "Feature 16:\n",
      "  Test file: 0.5978\n",
      "  Teen avg:  0.5076\n",
      "  Difference: 0.0901\n",
      "Feature 17:\n",
      "  Test file: -0.4940\n",
      "  Teen avg:  -0.0598\n",
      "  Difference: 0.4342\n",
      "Feature 18:\n",
      "  Test file: 1.8131\n",
      "  Teen avg:  -0.0559\n",
      "  Difference: 1.8691\n",
      "Feature 19:\n",
      "  Test file: 0.6193\n",
      "  Teen avg:  -0.3045\n",
      "  Difference: 0.9239\n",
      "Feature 20:\n",
      "  Test file: -2.3884\n",
      "  Teen avg:  -0.4916\n",
      "  Difference: 1.8968\n",
      "Feature 21:\n",
      "  Test file: 0.7397\n",
      "  Teen avg:  -0.1169\n",
      "  Difference: 0.8566\n",
      "\n",
      "Predicted age group: teens\n"
     ]
    }
   ],
   "source": [
    "# def extract_features(audio_path, sampling_rate=48000, duration=5.0):\n",
    "#     \"\"\"Extract features exactly matching training parameters\"\"\"\n",
    "#     # Load and preprocess audio\n",
    "#     audio, sr = librosa.load(audio_path, sr=sampling_rate, duration=duration)\n",
    "    \n",
    "#     # Normalize audio (as in training)\n",
    "#     audio = librosa.util.normalize(audio)\n",
    "    \n",
    "#     features = list()\n",
    "    \n",
    "#     # Extract spectral features with exact same parameters\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512))\n",
    "    \n",
    "#     features.extend([spectral_centroid, spectral_bandwidth, spectral_rolloff])\n",
    "    \n",
    "#     # Extract MFCCs with exact same parameters as training\n",
    "#     mfcc = librosa.feature.mfcc(\n",
    "#         y=audio, \n",
    "#         sr=sr, \n",
    "#         n_mfcc=20,\n",
    "#         n_fft=2048,\n",
    "#         hop_length=512,\n",
    "#         n_mels=128\n",
    "#     )\n",
    "    \n",
    "#     # Add mean of each MFCC coefficient (as in training)\n",
    "#     features.extend([np.mean(m) for m in mfcc])\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# def analyze_test_file(test_path, teen_avg):\n",
    "#     \"\"\"Analyze test file with synchronized parameters\"\"\"\n",
    "#     # Extract features with synchronized parameters\n",
    "#     test_features = extract_features(test_path)\n",
    "    \n",
    "#     # Scale features using the same scaler\n",
    "#     scaled_test = scaler.transform([test_features])\n",
    "#     selected_test = f_selector.transform(scaled_test)\n",
    "    \n",
    "#     print(\"\\nFeature values comparison:\")\n",
    "#     for i in range(len(selected_test[0])):\n",
    "#         print(f\"Feature {i}:\")\n",
    "#         print(f\"  Test file: {selected_test[0][i]:.4f}\")\n",
    "#         print(f\"  Teen avg:  {teen_avg[i]:.4f}\")\n",
    "#         print(f\"  Difference: {abs(selected_test[0][i] - teen_avg[i]):.4f}\")\n",
    "    \n",
    "#     return selected_test\n",
    "\n",
    "# # Test with synchronized parameters\n",
    "# test_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "\n",
    "# print(\"Processing with synchronized parameters...\")\n",
    "# teen_avg = analyze_training_data()\n",
    "# test_features = analyze_test_file(test_path, teen_avg)\n",
    "\n",
    "# # Make prediction\n",
    "# pred = model.predict(test_features)\n",
    "# print(f\"\\nPredicted age group: {encoder.inverse_transform(pred)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b6c57ca-789d-45c5-82ea-f10ca8887871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with final synchronized parameters...\n",
      "\n",
      "Predicted age group: thirties\n"
     ]
    }
   ],
   "source": [
    "# def extract_features_final(audio_path, sampling_rate=48000, duration=5.0):\n",
    "#     \"\"\"Final attempt with exact teen voice parameters\"\"\"\n",
    "    \n",
    "#     # Load audio with exact parameters\n",
    "#     audio, sr = librosa.load(audio_path, sr=sampling_rate, duration=duration)\n",
    "    \n",
    "#     # Trim silence and normalize exactly as in training\n",
    "#     audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "#     audio = librosa.util.normalize(audio)\n",
    "    \n",
    "#     # Ensure fixed length (important for consistent features)\n",
    "#     target_length = int(sampling_rate * duration)\n",
    "#     if len(audio) < target_length:\n",
    "#         audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "#     else:\n",
    "#         audio = audio[:target_length]\n",
    "    \n",
    "#     features = list()\n",
    "    \n",
    "#     # Extract spectral features with exact window sizes\n",
    "#     spectral_centroid = np.mean(librosa.feature.spectral_centroid(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512,\n",
    "#         window='hann', center=True))\n",
    "    \n",
    "#     spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512,\n",
    "#         window='hann', center=True))\n",
    "    \n",
    "#     spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(\n",
    "#         y=audio, sr=sr, n_fft=2048, hop_length=512,\n",
    "#         window='hann', center=True))\n",
    "    \n",
    "#     features.extend([spectral_centroid, spectral_bandwidth, spectral_rolloff])\n",
    "    \n",
    "#     # Extract MFCCs with exact teen voice parameters\n",
    "#     mfcc = librosa.feature.mfcc(\n",
    "#         y=audio, \n",
    "#         sr=sr, \n",
    "#         n_mfcc=20,\n",
    "#         n_fft=2048,\n",
    "#         hop_length=512,\n",
    "#         n_mels=128,\n",
    "#         window='hann',\n",
    "#         center=True,\n",
    "#         power=2.0\n",
    "#     )\n",
    "    \n",
    "#     # Use exact same feature statistics\n",
    "#     features.extend([np.mean(m) for m in mfcc])\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # Test with final synchronized parameters\n",
    "# test_path = \"/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/archive/cv-valid-train/cv-valid-train/sample-000088.mp3\"\n",
    "\n",
    "# print(\"Processing with final synchronized parameters...\")\n",
    "# test_features = extract_features_final(test_path)\n",
    "# scaled_test = scaler.transform([test_features])\n",
    "# selected_test = f_selector.transform(scaled_test)\n",
    "\n",
    "# # Make prediction\n",
    "# pred = model.predict(selected_test)\n",
    "# print(f\"\\nPredicted age group: {encoder.inverse_transform(pred)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71b215-d197-4d8f-bbf1-d5943076810a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ec718-c98e-4cf2-8bf2-d52840fa9eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a6d21b7-52a4-416b-8a66-15e62a2ddc54",
   "metadata": {},
   "source": [
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/10 (eng).wav      thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/10 (eng)_resampled_48000hz.wav     thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12  (eng).wav      thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12 (eng) -1.wav    fifties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12 (eng) -3.wav    fifties\n",
    "   \n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12 (eng) -5.wav    fifties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12 (eng) -6.wav    fifties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12 eng -2.wav     thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 13 (eng) -4.wav     thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 13 (eng) -5.wav     thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/6 eng 6.wav       thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/7 eng 7.wav      thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/9 eng 8.wav    thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/class 1 7 eng 1.wav      twenties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/class 1 10 (eng) 1.wav   thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/class 1 eng 2.wav      fifties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/eng 7 4.wav    thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/eng 9 3.wav      twenties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khamabari 13 (eng) 18.wav     fifties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 11 (eng 2) 15.wav     twenties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 11 (eng) -06.wav    thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 11 (eng) -9.wav     thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 11 (eng) -10.wav     thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 11 (eng) 16.wav    thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/Khanabari 12 ( eng) -12.wav   thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 ( eng) -15.wav    fifties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 ( eng) 17.wav     twenties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 ( eng) 22.wav    thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) - 14.wav       thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) -3.wav       thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) -4.wav       twenties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) -05.wav    twenties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) -07.wav       thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) -08.wav         thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) -11.wav    thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) -13.wav    thirties\n",
    "\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng) 20.wav    thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khanabari 12 (eng).wav    thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/khnabari 13 (eng) 31.wav      thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 10 (eng) 4.wav      thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 11 (eng) 1.wav     twenties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 11 (eng) 02.wav    thirties\n",
    "\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 11 (eng) 3.wav   thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 11 (eng) 5.wav       thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 11 (eng) 6.wav          thirties\n",
    "\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 11 (eng) 10.wav      thirties\n",
    "\n",
    "    /Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 11 (eng) 12.wav    twenties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 12 (eng) 7.wav     thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 12 (eng) 8.wav     thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 12 (eng) 9.wav   thirties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 12 (eng) 11.wav     twenties\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/teligati 12 (eng) 12.wav        thirties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb1eff-a08d-4d0a-a1ee-f1fa7945ce8d",
   "metadata": {},
   "source": [
    "Teens:\n",
    "\n",
    "/Users/hafsa_tazrian/Desktop copy/3-2/Lab/System Project/Voice wav format/kuet 12  (eng).wav\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
